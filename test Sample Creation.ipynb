{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f49b933b",
   "metadata": {},
   "source": [
    "# About\n",
    "Creating samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba4cef7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection with MySQL database is ready!\n"
     ]
    }
   ],
   "source": [
    "%run \"main_global.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78a76cb",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d000934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample_creation():\n",
    "    \"\"\"\n",
    "    Note: This class has parameters that fixes the sample size to weekly objects (24h x 7 days) using SQL queries.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, sqlq, test_frac_split, group_size = 24 * 7):\n",
    "        \"\"\"\n",
    "        \n",
    "        Input:\n",
    "        * test_frac_split: Fraction for the train test to keep from the trimmed dataset [0,1]\n",
    "        * group_size: Fixed in size to consider sample size of 24h and 7 days.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Intial parameters\n",
    "        self.sqlq = sqlq\n",
    "        self.test_frac_split = test_frac_split\n",
    "        self.group_size = group_size\n",
    "        \n",
    "        \n",
    "        # Define columns to drop:\n",
    "        # * nox has great correlation to no2, and no\n",
    "        # * rainf barely has any information.\n",
    "        self.cols_to_drop = [\"nox\", \"rainf\"]\n",
    "        \n",
    "    def trimmed_df(self):\n",
    "        \"\"\"\n",
    "        The dataset will trim off datetimes that don't fit within the standard WEEK definition [Saturday, Sunday]\n",
    "\n",
    "        Input:\n",
    "        * sqlq: Simple sql query. \n",
    "        \"\"\"\n",
    "        sqlq = self.sqlq\n",
    "        where_sql, table_sql, cols_sql = where_from_sqlq(sqlq), tablename_from_sqlq(sqlq), cols_from_sqlq(sqlq)\n",
    "\n",
    "        lower_aux_sqlq = \"SELECT datetime FROM {} {} AND dayname(datetime) = 'Sunday' and hour(datetime) = '00:00:00' ORDER BY datetime ASC limit 1\".format(table_sql, where_sql)\n",
    "        upper_aux_sqlq = \"SELECT datetime FROM {} {} AND dayname(datetime) = 'Saturday' and hour(datetime) = '23:00:00' ORDER BY datetime DESC limit 1\".format(table_sql, where_sql)\n",
    "\n",
    "        aux_sqlq = \"Select {} from {} where datetime >= ({}) and datetime <= ({}) ORDER BY DATETIME ASC\".format(\", \".join(cols_sql), table_sql, lower_aux_sqlq, upper_aux_sqlq)\n",
    "        \n",
    "        df = DataFrame(aux_qdata(aux_sqlq), columns = cols_sql)\n",
    "        \n",
    "        if not(self.cols_to_drop[0] == ''):\n",
    "            df = df.drop(self.cols_to_drop, axis = 1)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def trimmed_train_test_frac_split(self):\n",
    "        \"\"\"\n",
    "        Function to split the trimmed dataset into train and test datasets keeping the necessary dimensions\n",
    "        for the sample creation. \n",
    "        \"\"\"\n",
    "        \n",
    "        df = self.trimmed_df()\n",
    "        n_samples = int(df.shape[0] / self.group_size)\n",
    "        \n",
    "        train_shape = floor(n_samples * self.test_frac_split) * self.group_size\n",
    "        trim_train = df.loc[0:train_shape-1,:]\n",
    "        trim_test = df.loc[train_shape:,:]\n",
    "        \n",
    "        return trim_train.set_index(\"datetime\"), trim_test.set_index(\"datetime\")\n",
    "    \n",
    "    def samples(self):\n",
    "        trim_train, trim_test = self.trimmed_train_test_frac_split()\n",
    "        \n",
    "        train = array(split(trim_train, len(trim_train)/self.group_size))\n",
    "        test =  array(split(trim_test,  len(trim_test)/self.group_size))\n",
    "        \n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8fb3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlq = \"Select * from sima_station_MVI_MICE_CE WHERE datetime >=\\'2020-04-17 23:00:00\\'\"\n",
    "test_frac_split = 0.75\n",
    "total_groups = 35\n",
    "init_samples = sample_creation(sqlq, test_frac_split)\n",
    "trim_train, trim_test = init_samples.trimmed_train_test_frac_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b79b4",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b58ab846",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvi_method = \"MVI_MICE\"\n",
    "station = \"CE\"\n",
    "\n",
    "target = \"pm25\"\n",
    "look_back = 24 * 5\n",
    "look_forward = 24 * 2\n",
    "test_frac_split = 0.75\n",
    "\n",
    "colums_to_drop = [\"nox\", \"rainf\"]\n",
    "n_batches = 40\n",
    "\n",
    "leading_times = [1, 2, 3, 4, 5, 6, 12, 24, 48]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a24e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlq = \"Select * from sima_station_{}_{} WHERE datetime >=\\'2020-04-17 23:00:00\\'\".format(mvi_method, station)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27e3443",
   "metadata": {},
   "source": [
    "# UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33c3192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample_creation_time_metadata():\n",
    "    def __init__(self, target, look_back, look_forward, test_frac_split, sqlq, colums_to_drop, n_batches):\n",
    "        \n",
    "        # Parameters\n",
    "        self.target = target\n",
    "        self.look_back = look_back\n",
    "        self.look_forward = look_forward\n",
    "        self.test_frac_split = test_frac_split\n",
    "        \n",
    "        # Columns to drop from final dataset\n",
    "        self.colums_to_drop = colums_to_drop\n",
    "        \n",
    "        # Recognizing parameters from sql command\n",
    "        self.sqlq = sqlq\n",
    "        self.where_sql = where_from_sqlq(sqlq) \n",
    "        self.table_sql = tablename_from_sqlq(sqlq)\n",
    "        self.cols_sql = cols_from_sqlq(sqlq)\n",
    "        \n",
    "        # Number of batches\n",
    "        self.n_batches = n_batches\n",
    "        \n",
    "        # Counting total number of observations from sql command\n",
    "        count_obj = aux_qdata(\"select count(*) from ({}) s1\".format(sqlq))[0][0]\n",
    "        self.count_obj = count_obj\n",
    "        \n",
    "        # Calculating total number of observations for each batch\n",
    "        self.n_observations = int((count_obj - (count_obj % n_batches)) / n_batches)\n",
    "\n",
    "    def updated_table_time_metadata(self):\n",
    "        # Recognizing parameters from sql command\n",
    "        where_sql = self.where_sql\n",
    "        table_sql = self.table_sql\n",
    "        cols_sql = self.cols_sql\n",
    "        \n",
    "        # Counting total number of observations from sql command\n",
    "        count_obj = self.count_obj\n",
    "\n",
    "        # Calculating total number of observations for each batch\n",
    "        n_observations = self.n_observations\n",
    "\n",
    "        # Table with added time metadata\n",
    "        time_cols = \", \".join(cols_sql).replace(\"datetime\", \"datetime, monthname(datetime), hour(datetime)\")\n",
    "        time_sqlq = \"\"\" Select {}\n",
    "        from {} \n",
    "        {}\n",
    "        ORDER BY DATETIME ASC\n",
    "        LIMIT {}\n",
    "        \"\"\".format(time_cols, table_sql, where_sql, n_batches * n_observations)\n",
    "\n",
    "        # Editing column names \n",
    "        cols_sql.insert(1, \"month\")\n",
    "        cols_sql.insert(2, \"hour\")\n",
    "        time_df = DataFrame(aux_qdata(time_sqlq), columns = cols_sql)\n",
    "        \n",
    "        # Update output removing undesired columns\n",
    "        time_df = time_df.loc[:, ~time_df.columns.isin(self.colums_to_drop)]\n",
    "        \n",
    "        # Rearrange target to be at the end of the dataset\n",
    "        target_col = time_df.pop(self.target)\n",
    "        time_df[self.target] = target_col\n",
    "        \n",
    "        return time_df.set_index(\"datetime\")\n",
    "    \n",
    "    def encoding_df(self):\n",
    "        from category_encoders import LeaveOneOutEncoder\n",
    "        \n",
    "        time_df = self.updated_table_time_metadata()\n",
    "        \n",
    "        encoder = LeaveOneOutEncoder(cols = [\"month\", \"hour\"])\n",
    "        encoded_df = encoder.fit_transform(time_df, time_df[self.target])\n",
    "        \n",
    "        # |print(encoded_df.shape)\n",
    "        \n",
    "        return encoded_df \n",
    "    \n",
    "    \n",
    "    def normalizing_df(self):\n",
    "        from sklearn.preprocessing import RobustScaler\n",
    "        \n",
    "        # Calling encoded dataset\n",
    "        enc_df = self.encoding_df()\n",
    "        \n",
    "        # Backup objects\n",
    "        indices = enc_df.index\n",
    "        cols = enc_df.columns\n",
    "        target_col = enc_df[self.target].copy()\n",
    "        \n",
    "        transformer = RobustScaler(with_centering = False).fit(enc_df)\n",
    "        norm_np = transformer.transform(enc_df)\n",
    "        norm_df = DataFrame(norm_np, columns = cols).set_index(indices)\n",
    "        \n",
    "        # print(norm_df.shape)\n",
    "        \n",
    "        return norm_df\n",
    "\n",
    "    def X_y_sets(self):\n",
    "        \"\"\"\n",
    "        Creating samples for X, y datasets by splitting the multivariate dataset.\n",
    "\n",
    "        Input:\n",
    "        * df: This function takes in a pandas dataset for facility, but returns a numpy array of values\n",
    "        * look_back:\n",
    "        * look_forward: Multi-step\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.normalizing_df()\n",
    "        df_values = df.values\n",
    "        look_back = self.look_back\n",
    "        look_forward = self.look_forward\n",
    "\n",
    "        X, y = list(), list()\n",
    "\n",
    "        limit = len(df_values)\n",
    "\n",
    "        for i in range(limit):\n",
    "\n",
    "            # Updated indices delimiting end of series. \n",
    "            end_ix = i + look_back\n",
    "            out_end_ix = end_ix + look_forward-1\n",
    "\n",
    "            # check if we are beyond the dataset\n",
    "            if out_end_ix > limit:\n",
    "                break\n",
    "\n",
    "            # gather input and output parts of the pattern\n",
    "            seq_x = df_values[i:end_ix           , :-1]\n",
    "            seq_y = df_values[end_ix-1:out_end_ix, -1]\n",
    "\n",
    "            X.append(seq_x)\n",
    "            y.append(seq_y)\n",
    "            \n",
    "        X = array(X)\n",
    "        y = array(y)   \n",
    "\n",
    "        #print(\"X.shape\" , X.shape) \n",
    "        #print(\"y.shape\" , y.shape)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    \n",
    "    def train_test_sets(self):\n",
    "        \n",
    "        X, y = self.X_y_sets()\n",
    "        \n",
    "        split_point = floor(self.n_observations * self.n_batches * self.test_frac_split) \n",
    "        \n",
    "        train_X = X[:split_point, :] \n",
    "        train_y = y[:split_point, :]\n",
    "        test_X = X[split_point:, :] \n",
    "        test_y  = y[split_point:, :]\n",
    "        \n",
    "        print(\"train_X.shape\", train_X.shape)\n",
    "        print(\"train_y.shape\", train_y.shape)\n",
    "        print(\"test_X.shape\", test_X.shape)\n",
    "        print(\"test_y.shape\", test_y.shape)\n",
    "        \n",
    "        return train_X , train_y, test_X , test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b07b37b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape (13140, 120, 14)\n",
      "train_y.shape (13140, 48)\n",
      "test_X.shape (4214, 120, 14)\n",
      "test_y.shape (4214, 48)\n"
     ]
    }
   ],
   "source": [
    "init_sc = sample_creation_time_metadata(target, look_back, look_forward, test_frac_split, sqlq, colums_to_drop, n_batches)\n",
    "train_X , train_y, test_X , test_y = init_sc.train_test_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b00de09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421.4"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4214 / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2b399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = init_samples.samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82256d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aea112",
   "metadata": {},
   "outputs": [],
   "source": [
    "480"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
