{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888d1f65",
   "metadata": {},
   "source": [
    "# About\n",
    "Hyperparameter optimization is required to get the most out of your machine learning models.\n",
    "\n",
    "Hyperparameters are points of choice or configuration that allow a machine learning model to be customized for a specific task or dataset.\n",
    "\n",
    "Parameters are different from hyperparameters. Parameters are learned automatically; hyperparameters are set manually to help guide the learning process.\n",
    "\n",
    "Choosing a hyperparameter grid is probably the most difficult part of hyperparameter tuning: it's nearly impossible ahead of time to say which values of hyperparameters will work well and the optimal settings will depend on the dataset. Moreover, the hyperparameters have complex interactions with each other which means that just tuning one at a time doesn't work because when we start changing other hyperparameters that will affect the one we just tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ac34c",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e67215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection with MySQL database is ready!\n"
     ]
    }
   ],
   "source": [
    "%run \"../../main_global.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aba5cd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91cc70f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models\n",
    "import joblib\n",
    "\n",
    "# Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "# Hypertuning tools\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import SCORERS\n",
    "\n",
    "# Nonlinear models\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "# Ensemble models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Random seed\n",
    "seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca43e8",
   "metadata": {},
   "source": [
    "# UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d909e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multivariate_samples(object):\n",
    "    \"\"\"\n",
    "    Sequential processing of data to obtain time series.\n",
    "    \n",
    "    Activities:\n",
    "    - initial_df: Read SQL dataset for specific station number.\n",
    "    - samples_creation: Creation of samples array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, table_name, target, cols = '*', where = \"\"):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        * station_number: Database station number to process\n",
    "        \"\"\"\n",
    "        self.table_name = table_name\n",
    "        self.cols = cols\n",
    "        self.where = where\n",
    "        self.target = target\n",
    "        \n",
    "    def initial_df(self):\n",
    "        # Read raw dataset components from SQL database\n",
    "        sql_df = qdata(\"Select {} from {} {}\".format(self.cols, self.table_name, self.where))\n",
    "        \n",
    "        if self.cols == '*':\n",
    "            col_names = [i[0] for i in qdata(\"show columns from {}\".format(self.table_name))]\n",
    "        else: \n",
    "            col_names = self.cols.split(', ')\n",
    "\n",
    "        # Create dataframe\n",
    "        df = pd.DataFrame(sql_df)\n",
    "        df.columns = col_names\n",
    "\n",
    "        # Set `datetime` column as dataframe index\n",
    "        df = df.set_index('datetime')\n",
    "        df.sort_index(inplace=True)\n",
    "        \n",
    "        # Save temporary array with unmodified target information\n",
    "        target_arr = df[self.target]\n",
    "        \n",
    "        # Data normalization\n",
    "        df=(df-df.min())/(df.max()-df.min())\n",
    "        df = df.fillna(0)\n",
    "        df[self.target] = target_arr\n",
    "\n",
    "        # Overview\n",
    "        return df\n",
    "    \n",
    "    def samples_creation(self, n_steps, target_name):\n",
    "        \"\"\"\n",
    "        Transformation of Dataframe object into numpy.ndarray objects (input, output)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Rearrangin dataset to place target as last column\n",
    "        df = self.initial_df()\n",
    "        \n",
    "        target_col = df[target_name]\n",
    "\n",
    "        df = df.loc[:, df.columns != target_name]\n",
    "        df[target_name] = target_col     \n",
    "        \n",
    "        arr = df.to_numpy()\n",
    "        del target_col\n",
    "        \n",
    "        # Creating samples\n",
    "        tmp = list(reversed(range(len(arr)+1)))\n",
    "        tmp = tmp[:-n_steps][::-1]\n",
    "        tmp = pd.DataFrame(tmp).reset_index(drop = False)\n",
    "        tmp.columns = [\"index\", \"end_ix\"]\n",
    "        \n",
    "        # Create empty lists \n",
    "        X, y = list(), list()\n",
    "\n",
    "        for i, end_ix in zip(tmp[\"index\"], tmp[\"end_ix\"]):\n",
    "            \n",
    "            # Gather input and output parts of the pattern\n",
    "            seq_x, seq_y = arr[i:end_ix, :-1], arr[end_ix-1, -1]\n",
    "            X.append(seq_x)\n",
    "            y.append(seq_y)        \n",
    "        \n",
    "        return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b56cbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning(name, model, space, X, y):\n",
    "    # The searching algorithm includes a “cv” argument that allows:\n",
    "    # a) An integer number of folds to be specified, e.g. 5\n",
    "    #cross_val = 5\n",
    "    # b) A configured cross-validation object.\n",
    "    kfold = KFold(n_splits=3, shuffle=False)\n",
    "\n",
    "    # The scoring metric must be maximizing, meaning better models result in larger scores.\n",
    "    scoring_metric = 'neg_mean_squared_error'\n",
    "\n",
    "    # Search for best hyperparameters\n",
    "    grid = RandomizedSearchCV(estimator=model, \n",
    "                              param_distributions=search_space, \n",
    "                              cv=kfold, \n",
    "                              n_iter=100,\n",
    "                              scoring=scoring_metric)\n",
    "\n",
    "    result = grid.fit(X_test, y_test)\n",
    "    \n",
    "    # Save the trained model\n",
    "    filename = 'trained_ml_models_mvi/{}.sav'.format(name)\n",
    "    joblib.dump(result, filename)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab4a98bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘ml_trained_models’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "mkdir ml_trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab0eb7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a single model\n",
    "def single_model_evaluation(X_test, y_test, name):\n",
    "    # Load the trained model\n",
    "    filename = 'ml_trained_models/{}.sav'.format(name)\n",
    "    model = joblib.load(filename)\n",
    "\n",
    "    # make predictions\n",
    "    y_prediction = model.predict(X_test)\n",
    "    \n",
    "    metrics = dict()\n",
    "    # evaluate predictions\n",
    "    metrics[\"RMSE\"] = mean_squared_error(y_test, y_prediction, squared=False)\n",
    "    metrics[\"MAE\"] = mean_absolute_error(y_test, y_prediction)\n",
    "    metrics[\"MAPE (%)\"] = mean_absolute_percentage_error(y_test, y_prediction) *100\n",
    "    metrics[\"R^2 (%)\"] = r2_score(y_test, y_prediction) * 100\n",
    "    metrics[\"Max Error\"] = max_error(y_test, y_prediction)    \n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e53a3bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multivariate_samples(object):\n",
    "    \"\"\"\n",
    "    Sequential processing of data to obtain time series.\n",
    "    \n",
    "    Activities:\n",
    "    - initial_df: Read SQL dataset for specific station number.\n",
    "    - samples_creation: Creation of samples array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sqlq, target):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        * station_number: Database station number to process\n",
    "        \"\"\"\n",
    "        self.sqlq = sqlq\n",
    "        self.table_name = tablename_from_sqlq(sqlq)\n",
    "        self.cols = cols_from_sqlq(sqlq)\n",
    "        self.where = where_from_sqlq(sqlq)\n",
    "        self.target = target\n",
    "        \n",
    "    def initial_df(self):\n",
    "        # Read raw dataset components from SQL database\n",
    "        df = qdata(self.sqlq)\n",
    "\n",
    "        # Set `datetime` column as dataframe index\n",
    "        df = df.set_index('datetime')\n",
    "        df.sort_index(inplace=True)\n",
    "        \n",
    "        # Save temporary array with unmodified target information\n",
    "        target_arr = df[self.target]\n",
    "        \n",
    "        # Data normalization\n",
    "        df=(df-df.min())/(df.max()-df.min())\n",
    "        df = df.fillna(0)\n",
    "        df[self.target] = target_arr\n",
    "\n",
    "        # Overview\n",
    "        return df\n",
    "    \n",
    "    def samples_creation(self, n_steps):\n",
    "        \"\"\"\n",
    "        Transformation of Dataframe object into numpy.ndarray objects (input, output)\n",
    "        \"\"\"\n",
    "        target_name = self.target\n",
    "        \n",
    "        # Rearrangin dataset to place target as last column\n",
    "        df = self.initial_df()\n",
    "        \n",
    "        target_col = df[target_name]\n",
    "\n",
    "        df = df.loc[:, df.columns != target_name]\n",
    "        df[target_name] = target_col     \n",
    "        \n",
    "        arr = df.to_numpy()\n",
    "        del target_col\n",
    "        \n",
    "        # Creating samples\n",
    "        tmp = list(reversed(range(len(arr)+1)))\n",
    "        tmp = tmp[:-n_steps][::-1]\n",
    "        tmp = DataFrame(tmp).reset_index(drop = False)\n",
    "        tmp.columns = [\"index\", \"end_ix\"]\n",
    "        \n",
    "        # Create empty lists \n",
    "        X, y = list(), list()\n",
    "\n",
    "        for i, end_ix in zip(tmp[\"index\"], tmp[\"end_ix\"]):\n",
    "            \n",
    "            # Gather input and output parts of the pattern\n",
    "            seq_x, seq_y = arr[i:end_ix, :-1], arr[end_ix-1, -1]\n",
    "            X.append(seq_x)\n",
    "            y.append(seq_y)        \n",
    "        \n",
    "        return array(X), array(y), df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "412b46f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlq = \"SELECT * FROM sima_station_CE where datetime >=\\'2021-04-17 23:00:00\\'\"\n",
    "target = \"pm25\"\n",
    "\n",
    "init_mv = multivariate_samples(sqlq, target)\n",
    "\n",
    "X, y, _ = init_mv.samples_creation(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb4ba91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6808a8",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6668a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d664c86e",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca64d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
