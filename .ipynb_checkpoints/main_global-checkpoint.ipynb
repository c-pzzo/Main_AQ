{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3ca9ae",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd5dcccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0bbfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install PyYAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f4838d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mysql.connector\n",
    "import yaml\n",
    "from glob import iglob\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "from math import floor\n",
    "from numpy import array, split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6fbe2",
   "metadata": {},
   "source": [
    "# Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4ce982",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filename = \"mysql_credentials.yaml\"\n",
    "new_yaml_filnename = \"\"\n",
    "\n",
    "for i in range(5):\n",
    "    if not(os.path.exists(new_yaml_filnename)):\n",
    "        new_yaml_filnename = \"../\"*i + yaml_filename\n",
    "    else:\n",
    "        yaml_filename = new_yaml_filnename        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d149eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection with MySQL database is ready!\n"
     ]
    }
   ],
   "source": [
    "# For privacy reasons, user credentials are located in a separate .yaml file\n",
    "\n",
    "try:\n",
    "    mysql_credentials = yaml.safe_load(open(yaml_filename))\n",
    "except:\n",
    "    print('Login information is not available!!!')\n",
    "\n",
    "mysql_user = mysql_credentials[0]['mysql_user']\n",
    "mysql_pwd = mysql_credentials[0]['mysql_pwd']\n",
    "mysql_database_name = mysql_credentials[0]['mysql_database_name']\n",
    "print('Connection with MySQL database is ready!')\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\"\n",
    "  ,user=mysql_user\n",
    "  ,password=mysql_pwd\n",
    "  ,database='HDL_Project'\n",
    ")\n",
    "\n",
    "mycursor = mydb.cursor()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb2be7d",
   "metadata": {},
   "source": [
    "# User-Defined Functions / Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc4e76e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tablename_from_sqlq(sqlq):\n",
    "    \"\"\"\n",
    "    Extract tablename from sql query\n",
    "    \"\"\"\n",
    "    sqlq = sqlq.replace(\"FROM\", \"from\")\n",
    "    # Add a space at the end of query\n",
    "    txt = sqlq + \" \"\n",
    "    # Replace irrelevant characters\n",
    "    txt = txt.replace(\"`\", \"\")\n",
    "\n",
    "    # Obtain table name from sql table\n",
    "    index_from = txt.find(\"from\")\n",
    "    txt = txt[index_from + 5:]\n",
    "    txt = txt[:txt.find(\" \")]\n",
    "    \n",
    "    return txt    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac900853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cols_from_sqlq(sqlq):\n",
    "    sqlq = sqlq.replace(\"FROM\", \"from\")\n",
    "    sql_table = tablename_from_sqlq(sqlq)\n",
    "\n",
    "    if sqlq.find(\"*\") != -1:\n",
    "        col_list = aux_qdata(\"show columns from {}\".format(sql_table))\n",
    "        col_names = [col_list[i][0] for i in range(len(col_list))]\n",
    "\n",
    "    else:\n",
    "        \n",
    "        col_aux = sqlq[7:sqlq.find(\" from\")]\n",
    "        col_names = [col.strip() for col in col_aux.split(\",\")]\n",
    "        \n",
    "    return col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7476f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def where_from_sqlq(sqlq):\n",
    "    sqlq = sqlq.replace(\"WHERE\", \"where\")\n",
    "    \n",
    "    return sqlq[sqlq.find(\"where\"):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac7c5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://dev.mysql.com/doc/connector-python/en/connector-python-example-cursor-transaction.html\n",
    "\n",
    "def aux_qdata(sqlq):\n",
    "    \"\"\"\n",
    "    UDF to query data in raw format from a local MySQL RDBMS\n",
    "    \n",
    "    Input:\n",
    "    \"sqlq\": Query (e.g. Select * from table)\n",
    "    \n",
    "    \"\"\"\n",
    "    mycursor.execute(sqlq)\n",
    "    myresult = mycursor.fetchall()\n",
    "    return myresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1602438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qdata(sqlq):\n",
    "    \"\"\"\n",
    "    qdata queries data from MySQL RDBMS and returns it in a dataframe format\n",
    "    , along with its corresponding column names. \n",
    "    \n",
    "    Input:\n",
    "    * `sql_table`: Table name\n",
    "    * `sqlq`: Complete query (e.g. Select * from table where col1 = \"val1\")\n",
    "    (!) Don't rename columns here (e.g. \"Select col1 as colA ...\"). \n",
    "    \n",
    "    \"\"\"\n",
    "    col_names = cols_from_sqlq(sqlq)\n",
    "\n",
    "    data = DataFrame(aux_qdata(sqlq))\n",
    "    \n",
    "    data.columns = col_names\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36cad968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_restructuring():\n",
    "    \"\"\"\n",
    "    * Restructure_Raw_Data(): Function to restructure original raw data from SIMA to upload to MySQL database\n",
    "    * \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_location = \"Raw_SIMA_Data\"):\n",
    "        \n",
    "        self.file_location = \"/\" + file_location +  \"/\"\n",
    "        self.new_file_location = \"/Restructured_\" + self.file_location[1:]\n",
    "        \n",
    "        self.files_list = list(iglob('.{}*22.csv'.format(self.file_location)))\n",
    "\n",
    "        self.data_type = ['F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F']\n",
    "        self.cols = ['datetime','SE','NE','CE','NO','SO','NO2','NTE','NE2','SE2','SO2','SE3','SUR','NTE2','NE3']\n",
    "\n",
    "        self.F_types = ['DATETIME NOT NULL', 'FLOAT NULL', 'FLOAT NULL', 'FLOAT NULL', 'FLOAT NULL', 'FLOAT NULL', 'FLOAT NULL', 'FLOAT NULL', 'FLOAT NULL', 'FLOAT NULL', 'FLOAT NULL', 'FLOAT NULL', 'FLOAT NULL', 'FLOAT NULL', 'FLOAT NULL']\n",
    "        self.I_types = ['DATETIME NOT NULL', 'INT NULL', 'INT NULL', 'INT NULL', 'INT NULL', 'INT NULL', 'INT NULL', 'INT NULL', 'INT NULL', 'INT NULL', 'INT NULL', 'INT NULL', 'INT NULL', 'INT NULL', 'INT NULL']\n",
    "        \n",
    "    def Restructure_Raw_Data(self):\n",
    "\n",
    "        for file_to_edit in self.files_list:\n",
    "            # Editing original raw files\n",
    "            df = read_csv(file_to_edit)\n",
    "            file_location = self.file_location\n",
    "            new_file_location = self.new_file_location\n",
    "\n",
    "            # datetime is originally structured as DD/MM/YYYY and will be transformed to YYYY/MM/DD\n",
    "            day = df['date'].str.slice(0, 2)\n",
    "            month = df['date'].str.slice(3, 5)\n",
    "            year = df['date'].str.slice(6, 10)\n",
    "            time = df['date'].str[11:] + \":00\"\n",
    "\n",
    "            df[\"datetime_edit\"] = year + \"-\" + month + \"-\" + day + \" \" + time\n",
    "            df[\"datetime_edit\"] = df[\"datetime_edit\"].replace(\" a.m.\", \"\", regex=True)\n",
    "            df[\"datetime_edit\"] = df[\"datetime_edit\"].replace(\" p.m.\", \"\", regex=True)\n",
    "\n",
    "            df[\"date\"] = df[\"datetime_edit\"]\n",
    "            df = df.drop(\"datetime_edit\", axis = 1)    \n",
    "            df = df.rename(columns={\"date\": \"datetime\"})\n",
    "\n",
    "            file_to_edit = file_to_edit.replace(\"./\", \"\")\n",
    "\n",
    "            rename = file_to_edit\n",
    "            rename = rename.replace(self.file_location[1:], \"\")\n",
    "            rename = rename.replace(\"_2015_2022.csv\", \"\").lower()\n",
    "            rename = \"sima_\" + rename + \".csv\"\n",
    "            rename = self.new_file_location[1:] + rename\n",
    "\n",
    "            # Case-specific edit\n",
    "            rename = rename.replace(\"2.5\", \"25\")\n",
    "\n",
    "            df.to_csv(rename, encoding='utf-8', index=False)            \n",
    "            \n",
    "    \n",
    "    def sql_tables_structures(self, files_list, directory, prefix):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        with open(\"{}_sql_tables_structure.sql\".format(prefix),\"w\") as file:\n",
    "            # Query commands to create SQL tables.\n",
    "            for i in files_list:\n",
    "                file.write(\"CREATE TABLE {}.`{}` (\\n\".format(mysql_database_name, i))\n",
    "                for j, k, l in zip(range(len(files_list)), self.cols, self.data_type):\n",
    "                    if l == \"F\":\n",
    "                        dtypes = self.F_types\n",
    "                    else:\n",
    "                        dtypes = self.I_types\n",
    "\n",
    "                    if j == 0:\n",
    "                        file.write(\"`{}` {}\\n\".format(k, dtypes[j]))\n",
    "                    else:\n",
    "                        file.write(\", `{}` {}\\n\".format(k, dtypes[j]))\n",
    "                file.write(\") COMMENT = \\\"Source: ./{}/{}.csv\\\"\\n;\".format(directory,i))\n",
    "                file.write(\"\\n\")\n",
    "                \n",
    "    def upload_csv_script(self, files_list, directory, prefix):\n",
    "        # Command line. Insert CSV files into created MySQL tables\n",
    "        with open(\"{}_upload_csv.sh\".format(prefix),\"w\") as file:\n",
    "            file.write(\"#!/bin/bash\\n\")\n",
    "            \n",
    "            for i in files_list:\n",
    "                file.write(\"mysql -u{} -p{} --local-infile {} -e \\\"LOAD DATA LOCAL INFILE \\'./{}/{}.csv\\'  INTO TABLE {} FIELDS TERMINATED BY \\',\\' LINES TERMINATED BY \\'\\\\n\\' IGNORE 1 ROWS\\\"; \\n\".format(mysql_user, mysql_pwd, mysql_database_name, directory, i, i))\n",
    "\n",
    "                \n",
    "    def creating_views(self, files_list_trim, mvi_method_name, prefix):\n",
    "                \n",
    "        mvi_underscore = \"\" if mvi_method_name == \"\" else \"_\"\n",
    "        features = ['co', 'no', 'no2', 'nox', 'o3', 'pm10', 'pm25', 'so2', 'tout', 'wdr', 'wsr', 'rh', 'sr', 'rainf', 'prs']\n",
    "\n",
    "        stations = ['SE', 'NE', 'CE', 'NO', 'SO', 'NO2', 'NTE', 'NE2', 'SE2', 'SO2', 'SE3', 'SUR', 'NTE2', 'NE3']\n",
    "\n",
    "        with open(\"{}_views_creation.sql\".format(prefix),\"w\") as file:\n",
    "            for s in stations:\n",
    "                file.write(\"CREATE VIEW {}.sima_station{}{}_{} AS \\n\".format(mysql_database_name, mvi_underscore, prefix, s))\n",
    "                file.write(\"SELECT l0.datetime as datetime \\n\")\n",
    "\n",
    "                for f, ix in zip(features, range(len(features))):\n",
    "                    file.write(\", l{}.{} as {} \\n\".format(ix, s, f))\n",
    "\n",
    "                file.write(\"FROM {}.{}_sima_co l0 \\n\".format(mysql_database_name, prefix, f))\n",
    "\n",
    "                for f, ix in zip(features[1:], range(len(features))):\n",
    "                    file.write(\"LEFT JOIN {}.{}_sima_{} l{} ON l0.datetime = l{}.datetime \\n\".format(mysql_database_name, prefix, f, ix+1, ix+1))\n",
    "\n",
    "                file.write(\";\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3099c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample_creation_time_metadata():\n",
    "    def __init__(self, target, look_back, look_forward, test_frac_split, sqlq, colums_to_drop, n_batches):\n",
    "        \n",
    "        # Parameters\n",
    "        self.target = target\n",
    "        self.look_back = look_back\n",
    "        self.look_forward = look_forward\n",
    "        self.test_frac_split = test_frac_split\n",
    "        \n",
    "        # Columns to drop from final dataset\n",
    "        self.colums_to_drop = colums_to_drop\n",
    "        \n",
    "        # Recognizing parameters from sql command\n",
    "        self.sqlq = sqlq\n",
    "        self.where_sql = where_from_sqlq(sqlq) \n",
    "        self.table_sql = tablename_from_sqlq(sqlq)\n",
    "        self.cols_sql = cols_from_sqlq(sqlq)\n",
    "        \n",
    "        # Number of batches\n",
    "        self.n_batches = n_batches\n",
    "        \n",
    "        # Counting total number of observations from sql command\n",
    "        count_obj = aux_qdata(\"select count(*) from ({}) s1\".format(sqlq))[0][0]\n",
    "        self.count_obj = count_obj\n",
    "        \n",
    "        # Calculating total number of observations for each batch\n",
    "        self.n_observations = int((count_obj - (count_obj % n_batches)) / n_batches)\n",
    "\n",
    "    def updated_table_time_metadata(self):\n",
    "        # Recognizing parameters from sql command\n",
    "        where_sql = self.where_sql\n",
    "        table_sql = self.table_sql\n",
    "        cols_sql = self.cols_sql\n",
    "        \n",
    "        # Counting total number of observations from sql command\n",
    "        count_obj = self.count_obj\n",
    "\n",
    "        # Calculating total number of observations for each batch\n",
    "        n_observations = self.n_observations\n",
    "\n",
    "        # Table with added time metadata\n",
    "        time_cols = \", \".join(cols_sql).replace(\"datetime\", \"datetime, monthname(datetime), hour(datetime)\")\n",
    "        time_sqlq = \"\"\" Select {}\n",
    "        from {} \n",
    "        {}\n",
    "        ORDER BY DATETIME ASC\n",
    "        LIMIT {}\n",
    "        \"\"\".format(time_cols, table_sql, where_sql, n_batches * n_observations)\n",
    "\n",
    "        # Editing column names \n",
    "        cols_sql.insert(1, \"month\")\n",
    "        cols_sql.insert(2, \"hour\")\n",
    "        time_df = DataFrame(aux_qdata(time_sqlq), columns = cols_sql)\n",
    "        \n",
    "        # Update output removing undesired columns\n",
    "        time_df = time_df.loc[:, ~time_df.columns.isin(self.colums_to_drop)]\n",
    "        \n",
    "        # Rearrange target to be at the end of the dataset\n",
    "        target_col = time_df.pop(self.target)\n",
    "        time_df[self.target] = target_col\n",
    "        \n",
    "        return time_df.set_index(\"datetime\")\n",
    "    \n",
    "    def encoding_df(self):\n",
    "        from category_encoders import LeaveOneOutEncoder\n",
    "        \n",
    "        time_df = self.updated_table_time_metadata()\n",
    "        \n",
    "        encoder = LeaveOneOutEncoder(cols = [\"month\", \"hour\"])\n",
    "        encoded_df = encoder.fit_transform(time_df, time_df[self.target])\n",
    "        \n",
    "        # |print(encoded_df.shape)\n",
    "        \n",
    "        return encoded_df \n",
    "    \n",
    "    \n",
    "    def normalizing_df(self):\n",
    "        from sklearn.preprocessing import RobustScaler\n",
    "        \n",
    "        # Calling encoded dataset\n",
    "        enc_df = self.encoding_df()\n",
    "        \n",
    "        # Backup objects\n",
    "        indices = enc_df.index\n",
    "        cols = enc_df.columns\n",
    "        target_col = enc_df[self.target].copy()\n",
    "        \n",
    "        transformer = RobustScaler(with_centering = False).fit(enc_df)\n",
    "        norm_np = transformer.transform(enc_df)\n",
    "        norm_df = DataFrame(norm_np, columns = cols).set_index(indices)\n",
    "        \n",
    "        # print(norm_df.shape)\n",
    "        \n",
    "        return norm_df\n",
    "\n",
    "    def X_y_sets(self):\n",
    "        \"\"\"\n",
    "        Creating samples for X, y datasets by splitting the multivariate dataset.\n",
    "\n",
    "        Input:\n",
    "        * df: This function takes in a pandas dataset for facility, but returns a numpy array of values\n",
    "        * look_back:\n",
    "        * look_forward: Multi-step\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.normalizing_df()\n",
    "        df_values = df.values\n",
    "        look_back = self.look_back\n",
    "        look_forward = self.look_forward\n",
    "\n",
    "        X, y = list(), list()\n",
    "\n",
    "        limit = len(df_values)\n",
    "\n",
    "        for i in range(limit):\n",
    "\n",
    "            # Updated indices delimiting end of series. \n",
    "            end_ix = i + look_back\n",
    "            out_end_ix = end_ix + look_forward-1\n",
    "\n",
    "            # check if we are beyond the dataset\n",
    "            if out_end_ix > limit:\n",
    "                break\n",
    "\n",
    "            # gather input and output parts of the pattern\n",
    "            seq_x = df_values[i:end_ix           , :-1]\n",
    "            seq_y = df_values[end_ix-1:out_end_ix, -1]\n",
    "\n",
    "            X.append(seq_x)\n",
    "            y.append(seq_y)\n",
    "            \n",
    "        X = array(X)\n",
    "        y = array(y)   \n",
    "\n",
    "        #print(\"X.shape\" , X.shape) \n",
    "        #print(\"y.shape\" , y.shape)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    \n",
    "    def train_test_sets(self):\n",
    "        \n",
    "        X, y = self.X_y_sets()\n",
    "        \n",
    "        split_point = floor(self.n_observations * self.n_batches * self.test_frac_split) \n",
    "        \n",
    "        train_X = X[:split_point, :] \n",
    "        train_y = y[:split_point, :]\n",
    "        test_X = X[split_point:, :] \n",
    "        test_y  = y[split_point:, :]\n",
    "        \n",
    "        print(\"train_X.shape\", train_X.shape)\n",
    "        print(\"train_y.shape\", train_y.shape)\n",
    "        print(\"test_X.shape\", test_X.shape)\n",
    "        print(\"test_y.shape\", test_y.shape)\n",
    "        \n",
    "        return train_X , train_y, test_X , test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e360493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del mysql_credentials, mysql_user, mysql_pwd, mysql_database_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
